\documentclass[11pt]{article}
\usepackage[english]{babel}
\linespread{1.0}
\usepackage{slantsc}
\usepackage[hmargin=1in,vmargin=3cm]{geometry}
%\input{./bild_mk2.sty}
\input{./default.sty}
\usepackage[largesc]{newtxtext}
\usepackage[scaled=0.85]{DejaVuSansMono}
\usepackage[vvarbb, nosymbolsc]{newtxmath}
\usepackage{multicol}
\usepackage[cal=esstix, scr=rsfso]{mathalfa}
\usepackage{bm}
\usepackage{verbatim}
%\numberwithin{equation}{section}
\renewcommand{\arraystretch}{1.0}
%\usepackage{titlecaps}
%\usepackage[explicit]{titlesec}
%\usepackage{titletoc}
%\titleformat{\section}{\Large\sffamily\bfseries}{\thesection. }{1.5ex}{#1}
%\titleformat{\subsection}{\large\sffamily\bfseries}{\thesubsection. }{1.0ex}{#1}
%\titleformat{\subsubsection}{\sffamily\bfseries}{\thesubsubsection\ }{0.5ex}{#1}
%\titlespacing*{\section}{0em}{1.0em}{0.5em}
%\titlespacing*{\subsection}{0em}{1.0em}{0.5em}
%\titlespacing*{\subsubsection}{0pt}{0.5em}{0.5em}
%
%\pagestyle{fancyplain}
%\renewcommand{\headrulewidth}{0pt}
%\fancyhf{}
%\fancyfoot[R]{\thepage}
%\renewcommand{\title}[3]{\begin{flushleft}\LARGE\sffamily\bfseries{#1}\\[1ex]
%    \normalfont\sffamily \large #2 \hfill \normalsize #3 \end{flushleft}}
%
%\usepackage{hyperref}
%\hypersetup{colorlinks=true,linkcolor=Black,citecolor=Black,urlcolor=MplBlue}
%\newcommand{\ds}{\displaystyle}
%\renewcommand{\vec}[1]{\harpoonacc{#1}}
%\renewcommand{\t}{\dagger}
%\newcommand{\cip}[2]{\left(#1\middle|#2\right)\:}
%\newcommand{\cmel}[3]{\left(#1 \middle| #2 \middle| #3 \right)\:}
%\newcommand{\Coul}{\mathcal{J}}
%\newcommand{\Exch}{\mathcal{K}}
%\newcommand{\coverb}[2]{\overbrace{#1}^{\mathclap{#2}}}
%\newcommand{\cunderb}[2]{\underbrace{#1}_{\mathclap{#2}}}
\begin{document}
\title{The SCF Workshop Notes 3}{Lucas Aebersold}{\today}
\setcounter{section}{2}
\section{Direct Inversion of the Iterative Subspace}

When solving systems of linear (or nonlinear) equations, iterative methods are often employed. Unfortunately, such methods often suffer from convergence issues such as numerical instability, slow convergence, and significant computational expense when applied to difficult problems. In these cases, we can apply convergence acceleration methods to both speed up, stabilize and/or reduce the cost for the convergence patterns of these methods, so that solving such problems become computationally tractable.  One such method is known as the direct inversion of the iterative subspace (DIIS) method, which is commonly applied to address convergence issues within self consistent field computations in Hartree-Fock theory (and other iterative electronic structure methods).  In this tutorial, we'll introduce the theory of DIIS for a general iterative procedure, before integrating DIIS into our previous implementation of RHF.

\subsection{Theory}

DIIS is a convergence acceleration method, which is applicable to numerous problems in linear algebra and the computational sciences, in particular quantum chemistry. Therefore, we will introduce the theory of this method in the general sense, before seeking to apply it to SCF.  

Suppose that for a given problem, there exist a set of trial vectors $\qty{\ket*{\mathbf{g}_i}}$ which have been generated iteratively, converging toward the true solution, $\ket*{\mathbf{\bar{g}}}$.  Then the true solution can be approximately constructed as a linear combination of the trial vectors,
$$ \ket{\mathbf{g}} = \sum_i c_i\ket*{\mathbf{g}_i} $$
where we require that the residual vector 
$$ \ket*{\mathbf{r}} = \sum_ic_i \ket*{\mathbf{r}_i} \qquad \ket*{\mathbf{r}_i} 
=\, \ket*{\mathbf{g}_{i+1}} - \ket*{\mathbf{g}_i}$$
is a least-squares approximate to the zero vector, according to the constraint
$$\sum_i c_i = 1.$$
This constraint on the expansion coefficients can be seen by noting that each trial function ${ \mathbf{g}}_i$ may be represented as an error vector applied to the true solution, $\ket*{\mathbf{\bar{g}}} + \ket*{\mathbf{e}_i}$.  Then
\begin{align}
\ket*{\mathbf{g}} &= \sum_ic_i\ket*{\mathbf{g}_i}\\
&= \sum_i c_i(\ket*{\mathbf{\bar{g}}} + \ket*{\mathbf{e}_i})\\
&= \ket*{\mathbf{\bar{g}}}\sum_i c_i + \sum_i c_i\ket*{\mathbf{e}_i}
\end{align}
Convergence results in a minimization of the error (causing the second term to vanish). In order for the DIIS solution vector $\ket{\mathbf{g}}$ and the true solution vector $\ket*{\mathbf{\bar{g}}}$ to be equal, it must be true that $\sum_i c_i = 1$.  We satisfy our condition for the residual vector by minimizing its norm,
$$ \ip{\mathbf{r}}{\mathbf{r}} = \sum_{ij} c_i^* c_j \ip*{\mathbf{r}_i}{\mathbf{r}_j} $$
using Lagrange's method of undetermined coefficients subject to the constraint on $\{c_i\}$:
$${\mathcal{L}} = {\mathbf{c}}^{\dagger}{\mathbf{Bc}} - \lambda\left(1 - \sum_i c_i\right)$$
where $B_{ij} = \ip*{\mathbf{r}_i }{\mathbf{r}_j} $ is the matrix of residual vector overlaps.  Minimization of the Lagrangian with respect to the coefficient $c_k$ yields (for real values)
\begin{align}
\pdv{\mathcal{L}}{c_k} = 0 &= \sum_j c_j B_{jk} + \sum_i c_i B_{ik} - \lambda\\
&= 2\sum_ic_iB_{ik} - \lambda
\end{align}
which has matrix representation
\begin{equation}
\begin{pmatrix}
B_{11} & B_{12} & \cdots & B_{1m} & -1 \\
B_{21} & B_{22} & \cdots & B_{2m} & -1 \\
\vdots  & \vdots  & \ddots & \vdots  & \vdots \\
B_{n1} & B_{n2} & \cdots & B_{nm} & -1 \\
-1 & -1 & \cdots & -1 & 0
\end{pmatrix}
\begin{pmatrix}
c_1\\
c_2\\
\vdots \\
c_n\\
\lambda
\end{pmatrix}
=
\begin{pmatrix}
0\\
0\\
\vdots\\
0\\
-1
\end{pmatrix}
\end{equation}
which is referred to as the Pulay equation, named after the inventor of DIIS.  It is worth noting that at this point our trial vectors, residual vectors, and solution vector may in fact be tensors of arbitrary rank. For this reason that we have used the generic notation of Dirac in the above discussion to denote the inner product between such objects.
\subsection{Algorithms for DIIS}
The general DIIS procedure, as described above, has the following structure during each iteration:
\subsubsection{A Generic DIIS procedure}
\begin{enumerate}
	\item  Compute new trial vector, $\ket*{\mathbf{g}_{i+1}}$, append to list of trial vectors
	\item  Compute new residual vector, $\ket*{\mathbf{r}_{i+1}}$, append to list of trial vectors
	\item  Check convergence criteria
	\begin{itemize}
		\item  If RMSD of $\ket*{\mathbf{r}_{i+1}}$ sufficiently small, and
		\item  If change in DIIS solution vector $\ket{\mathbf{g}}$ is sufficiently small, break
	\end{itemize}
	\item  Build $\mathbf{B}$ matrix from previous residual vectors
	\item  Solve Pulay equation for coefficients $\{c_i\}$
	\item  Compute DIIS solution vector $\ket{\mathbf{g}}$
\end{enumerate}
For SCF iteration, the most common choice of trial vector is the Fock matrix \verb|F|; this choice has the advantage over other potential choices (e.g., the density matrix $\mathbf{P}$) of $\mathbf{F}$ not being idempotent, so that it may benefit from extrapolation.  The residual vector is commonly chosen to be the orbital gradient in the AO basis,
\[g_{\mu\nu} = (\mathbf{F}\mathbf{P}\mathbf{S} -  \mathbf{S}\mathbf{P}\mathbf{F})_{\mu\nu}\]
however the better choice (which we will make in our implementation!) is to orthonormalize the basis of the gradient with the inverse overlap metric ${\mathbf{A}} = {\mathbf{S}}^{-1/2}$
\[r_{\mu\nu} = ({\mathbf{A}}^{\rm T}(\mathbf{F}\mathbf{P}\mathbf{S} -  \mathbf{S}\mathbf{P}\mathbf{F}){ \mathbf{A}})_{\mu\nu}\]
Therefore, the SCF-specific DIIS procedure (integrated into the SCF iteration algorithm) will be
\subsubsection{DIIS within an SCF Iteration}
\begin{enumerate}
	\item Compute  $\mathbf{F}$, append to list of previous trial vectors
	\item Compute AO orbital gradient \verb|r|, append to list of previous residual vectors
	\item Compute RHF energy
	\item Check convergence criteria
	\begin{itemize}
		\item If RMSD of $\bb{r}$ sufficiently small, and
		\item If change in SCF energy sufficiently small, break
	\end{itemize}
	\item Build  \verb|B| matrix from previous AO gradient vectors
	\item Solve Pulay equation for coefficients $\{c_i\}$
	\item Compute DIIS solution vector \verb|F_diis| from $\{c_i\}$ and previous trial vectors
	\item Compute new orbital guess with \verb|F_diis|
\end{enumerate}
Now we're ready to write our SCF iterations in accordance to the above procedure.
\subsection{Starting DIIS}
Since DIIS builds the approximate solution vector $\ket{\mathbf{g}}$ as a linear combination of the previous trial vectors $\qty{\ket{\mathbf{g}_i}}$, DIIS must start in the second SCF iteration. 
\subsubsection{Building B}
\begin{enumerate}
	\item The $\mathbf{B}$ matrix in the Lagrange equation is really $\tilde{\mathbf{B}} = \begin{pmatrix} {\mathbf{B}} & -1\\ -1 & 0\end{pmatrix}$.
	\item Since $\mathbf{B}$ is the matrix of residual overlaps, it will be a square matrix of dimension equal to the number of residual vectors.  If $\mathbf{B}$ is an $N\times N$ matrix, how big is $\tilde{\mathbf{B}}$?
	\item Since our residuals are real, $\mathbf{B}$ will be a symmetric matrix.
	\item To build $\tilde{\mathbf{B}}$, make an empty array of the appropriate dimension, then use array indexing to set the values of the elements.
\end{enumerate}
\subsubsection{Solving the Pulay equation}
\begin{enumerate}
	\item Use built-in NumPy functionality to make your life easier.
	\item The solution vector for the Pulay equation is $\tilde{\mathbf{c}} = \begin{pmatrix} {\mathbf{c}}\\ \lambda\end{pmatrix}$, where $\lambda$ is the Lagrange multiplier, and the right hand side is $\begin{pmatrix} {\bf 0}\\ -1\end{pmatrix}$.  
\end{enumerate}

\newpage

\section{Unrestricted Hartree Fock}
Here is the briefest of overviews for the unrestricted form of the SCF equations. Essentially we need to compute all previous matrix elements but for both the alpha and beta electrons, the coupled equations look like
\begin{align}
\mathbf{F}^{\alpha}\mathbf{C}^{\alpha} &= \mathbf{SC}^{\alpha}{\bm{\upvarepsilon}}^{\alpha} \\
\mathbf{F}^{\beta}\mathbf{C}^{\beta} &= \mathbf{SC}^{\beta}{\bm{\upvarepsilon}}^{\,\beta}
\end{align}
which are the unrestricted generalizations of the restricted Roothaan equations, called the Pople-Nesbitt equations. Here, the one-electron Fock matrices are given by
\begin{align}
F_{\mu\nu}^{\alpha} &= H_{\mu\nu} + (\mu\nu|\lambda\sigma)\,[P_{\lambda\sigma}^{\alpha} + P_{\lambda\sigma}^{\beta}] - \cip{\mu\lambda}{\nu\sigma} P_{\lambda\sigma}^{\beta}\\
F_{\mu\nu}^{\beta} &= H_{\mu\nu} + \cip{\mu\nu}{\lambda\sigma}[P_{\lambda\sigma}^{\alpha} + P_{\lambda\sigma}^{\beta}] - \cip{\mu\lambda}{\nu\sigma}P_{\lambda\sigma}^{\alpha}
\end{align}
where the density matrices $P_{\lambda\sigma}^{\alpha}$ and $P_{\lambda\sigma}^{\beta}$ are given by
\begin{align}
P_{\lambda\sigma}^{\alpha} &= C_{\sigma i}^{\alpha}C_{\lambda i}^{\alpha}\\
P_{\lambda\sigma}^{\beta} &= C_{\sigma i}^{\beta}C_{\lambda i}^{\beta}
\end{align}
Unlike for RHF, the orbital coefficient matrices $\mathbf{C}^{\alpha}$ and $\mathbf{C}^{\beta}$ are of dimension $M\times N^{\alpha}$ and $M\times N^{\beta}$, where $M$ is the number of AO basis functions and $N^{\alpha}$ ($N^{\beta}$) is the number of $\alpha$ ($\beta$) electrons.  The total UHF energy is given by
\begin{align}
E^{\text{UHF}}_{\text{total}} &= E^{\text{UHF}}_{e} + E^{\text{BO}}_{n}\qq{with}\\
E^{\text{UHF}}_{e} &= \frac{1}{2}\qty[(\mathbf{P}^{\alpha} + \mathbf{P}^{\,\beta}){\bf H} + 
\mathbf{P}^{\alpha}\mathbf{F}^{\alpha} + \mathbf{P}^{\beta}\mathbf{F}^{\beta}].
\end{align}

\end{document}
